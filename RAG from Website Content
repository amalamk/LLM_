pip install sentence-transformers faiss-cpu transformers

import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import faiss
import numpy as np
import re

# Step 1: Scrape and clean text from a URL
def scrape_text_from_url(url):
    print(f"Scraping: {url}")
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Remove scripts and styles
    for script in soup(["script", "style"]):
        script.extract()

    # Get text
    text = soup.get_text()
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# Step 2: Chunk the text into small passages
def chunk_text(text, chunk_size=200):
    sentences = re.split(r'(?<=[.!?]) +', text)
    chunks = []
    current = ""
    for sentence in sentences:
        if len(current) + len(sentence) < chunk_size:
            current += " " + sentence
        else:
            chunks.append(current.strip())
            current = sentence
    if current:
        chunks.append(current.strip())
    return chunks

# Step 3: Embed and index the content
def build_index(chunks):
    print("Embedding text and building index...")
    embeddings = embedder.encode(chunks, convert_to_numpy=True)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index, chunks

# Step 4: Retrieve top-k chunks
def retrieve(query, index, chunks, top_k=3):
    query_vec = embedder.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)
    return [chunks[i] for i in indices[0]]

# Step 5: Generate answer using retrieved context
def generate_answer(query, context_chunks):
    context = " ".join(context_chunks)
    input_text = f"Context: {context}\nQuestion: {query}"
    result = generator(input_text, max_length=100, do_sample=False)[0]['generated_text']
    return result

# Load models
print("Loading models...")
embedder = SentenceTransformer('all-MiniLM-L6-v2')
generator = pipeline("text2text-generation", model="google/flan-t5-base")

# URL(s) to process
urls = [
    "https://en.wikipedia.org/wiki/Eiffel_Tower",
    "https://en.wikipedia.org/wiki/Paris"
]

# Main pipeline
all_chunks = []
for url in urls:
    text = scrape_text_from_url(url)
    chunks = chunk_text(text)
    all_chunks.extend(chunks)

index, chunks_db = build_index(all_chunks)

# Interactive Q&A loop
while True:
    query = input("\nAsk a question (or type 'exit'): ")
    if query.lower() == "exit":
        break
    top_chunks = retrieve(query, index, chunks_db)
    answer = generate_answer(query, top_chunks)
    print(f"\nAnswer: {answer}")



# ... [all the previous code stays the same until indexing] ...

# Ask a single question once
query = input("\nAsk a question: ")
top_chunks = retrieve(query, index, chunks_db)
answer = generate_answer(query, top_chunks)
print(f"\nAnswer: {answer}")

